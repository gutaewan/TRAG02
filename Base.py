import os
import re
import json
import time
import uuid
import hashlib
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple

import streamlit as st
import requests
import feedparser

# Optional: full article extraction
try:
    from bs4 import BeautifulSoup  # type: ignore
except Exception:
    BeautifulSoup = None  # type: ignore

try:
    import trafilatura  # type: ignore
except Exception:
    trafilatura = None  # type: ignore

from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama

from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.runnables.history import RunnableWithMessageHistory

# LangChain ë²„ì „ì— ë”°ë¼ import ê²½ë¡œê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ í˜¸í™˜ ì²˜ë¦¬
#try:
#    from langchain.chains import create_history_aware_retriever
#except Exception:
#    try:
#        from langchain.chains.history_aware_retriever import create_history_aware_retriever
#    except Exception:#
#        create_history_aware_retriever = None


# =========================
# 0) Config (./config/config.py)
#    - ë‹¨ì¼ ì„¤ì • ì†ŒìŠ¤: ./config/config.py
#    - BaseRAG_v3.py ë‚´ë¶€ í´ë˜ìŠ¤ë¡œ ì„¤ì •í•˜ì§€ ì•ŠìŒ
# =========================

CONFIG_DIR = "./config"
CONFIG_FILE = os.path.join(CONFIG_DIR, "config.py")

DEFAULTS: Dict[str, Any] = {
    # Directories
    "DATA_DIR": "./data",
    "NEWS_DIR": "./news_texts",
    "LOG_DIR": "./logs",
    "VECTOR_DB_ROOT": "./vector_db",

    # Vector DB
    "VECTOR_DB": "chroma",

    # Models
    "LLM_MODEL": "llama3.2",
    "EMBED_MODEL": "qwen3-embedding",

    # Chunking
    "CHUNK_SIZE": 1000,
    "CHUNK_OVERLAP": 200,

    # Periodic intervals
    "PDF_SYNC_INTERVAL_SEC": 600,
    "NEWS_CRAWL_INTERVAL_SEC": 600,

    # News
    "NEWS_KEYWORDS": ["ì†Œí”„íŠ¸ì›¨ì–´ ê³µí•™", "AI ì•ˆì „", "ìë™ì°¨ ê¸°ëŠ¥ì•ˆì „", "SDV"],
    "NEWS_MAX_ITEMS_PER_KEYWORD": 10,
    "NEWS_TIMEOUT_SEC": 20,

    # Auto refresh
    "AUTO_REFRESH_ENABLED": True,
    "AUTO_REFRESH_TICK_SEC": 30,

    # Safety: reset stuck "generating" state if a run is interrupted
    "GENERATION_STALE_SEC": 180,
}

TEMPLATE_CONFIG = """# TRAG01 ì„¤ì • íŒŒì¼\n# ì—¬ê¸° ê°’ì„ ìˆ˜ì •í•˜ë©´ BaseRAG_v3.pyê°€ ê·¸ëŒ€ë¡œ ë°˜ì˜í•©ë‹ˆë‹¤.\n\n# Directories\nDATA_DIR = \"./data\"\nNEWS_DIR = \"./news_texts\"\nLOG_DIR = \"./logs\"\nVECTOR_DB_ROOT = \"./vector_db\"\n\n# Vector DB\nVECTOR_DB = \"chroma\"\n\n# Models\nLLM_MODEL = \"llama3.2\"\nEMBED_MODEL = \"qwen3-embedding\"\n\n# Chunking\nCHUNK_SIZE = 1000\nCHUNK_OVERLAP = 200\n\n# Periodic intervals (seconds)\nPDF_SYNC_INTERVAL_SEC = 600\nNEWS_CRAWL_INTERVAL_SEC = 600\n\n# News\nNEWS_KEYWORDS = [\"ì†Œí”„íŠ¸ì›¨ì–´ ê³µí•™\", \"AI ì•ˆì „\", \"ìë™ì°¨ ê¸°ëŠ¥ì•ˆì „\", \"SDV\"]\nNEWS_MAX_ITEMS_PER_KEYWORD = 10\nNEWS_TIMEOUT_SEC = 20\n\n# Auto refresh\nAUTO_REFRESH_ENABLED = True\nAUTO_REFRESH_TICK_SEC = 30\n\n# Safety: reset stuck \"generating\" state if a run is interrupted\nGENERATION_STALE_SEC = 180\n"""


def ensure_config_file() -> None:
    os.makedirs(CONFIG_DIR, exist_ok=True)
    if not os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, "w", encoding="utf-8") as f:
            f.write(TEMPLATE_CONFIG)


def load_config() -> Dict[str, Any]:
    ensure_config_file()
    cfg = dict(DEFAULTS)
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("trag_user_config", CONFIG_FILE)
        if spec and spec.loader:
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)  # type: ignore
            for k in list(DEFAULTS.keys()):
                if hasattr(mod, k):
                    cfg[k] = getattr(mod, k)
    except Exception as e:
        print(f"[WARN] Failed to load config.py: {e}")
    # normalize types
    if isinstance(cfg.get("NEWS_KEYWORDS"), tuple):
        cfg["NEWS_KEYWORDS"] = list(cfg["NEWS_KEYWORDS"])
    return cfg


CFG: Dict[str, Any] = load_config()

# Ensure dirs
os.makedirs(str(CFG["DATA_DIR"]), exist_ok=True)
os.makedirs(str(CFG["NEWS_DIR"]), exist_ok=True)
os.makedirs(str(CFG["LOG_DIR"]), exist_ok=True)
os.makedirs(str(CFG["VECTOR_DB_ROOT"]), exist_ok=True)


# =========================
# =========================
# 0.5) Persistent storage for chats (survive browser refresh)
# =========================

CHAT_STORE_DIR = os.path.join(str(CFG["VECTOR_DB_ROOT"]), "chat_store")
os.makedirs(CHAT_STORE_DIR, exist_ok=True)

def _chat_registry_path() -> str:
    return os.path.join(CHAT_STORE_DIR, "chat_registry.json")

def _chat_messages_path(session_id: str) -> str:
    return os.path.join(CHAT_STORE_DIR, f"chat_{session_id}.json")

# =========================
# 1) Logging
# =========================
def _load_chat_registry_from_disk() -> Dict[str, str]:
    p = _chat_registry_path()
    if not os.path.exists(p):
        return {}
    try:
        with open(p, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict):
            # ensure string->string
            out: Dict[str, str] = {}
            for k, v in data.items():
                if isinstance(k, str) and isinstance(v, str):
                    out[k] = v
            return out
        return {}
    except Exception as e:
        logger.info(f"[CHAT_STORE] failed to load registry: {e}")
        return {}


def _save_chat_registry_to_disk(reg: Dict[str, str]) -> None:
    try:
        os.makedirs(CHAT_STORE_DIR, exist_ok=True)
        tmp = _chat_registry_path() + ".tmp"
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump(reg, f, ensure_ascii=False, indent=2)
        os.replace(tmp, _chat_registry_path())
    except Exception as e:
        logger.info(f"[CHAT_STORE] failed to save registry: {e}")


def _serialize_messages(msgs) -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []
    for m in msgs or []:
        try:
            # StreamlitChatMessageHistory uses dict-like messages with .type/.content
            t = getattr(m, "type", None) or m.get("type")
            c = getattr(m, "content", None) or m.get("content")
            if t and c is not None:
                out.append({"type": str(t), "content": str(c)})
        except Exception:
            continue
    return out


def _load_messages_from_disk(session_id: str) -> List[Dict[str, str]]:
    p = _chat_messages_path(session_id)
    if not os.path.exists(p):
        return []
    try:
        with open(p, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, list):
            out = []
            for it in data:
                if isinstance(it, dict) and "type" in it and "content" in it:
                    out.append({"type": str(it["type"]), "content": str(it["content"])})
            return out
        return []
    except Exception as e:
        logger.info(f"[CHAT_STORE] failed to load messages for {session_id}: {e}")
        return []


def _save_messages_to_disk(session_id: str, msgs) -> None:
    try:
        os.makedirs(CHAT_STORE_DIR, exist_ok=True)
        tmp = _chat_messages_path(session_id) + ".tmp"
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump(_serialize_messages(msgs), f, ensure_ascii=False, indent=2)
        os.replace(tmp, _chat_messages_path(session_id))
    except Exception as e:
        logger.info(f"[CHAT_STORE] failed to save messages for {session_id}: {e}")


def _hydrate_history_from_disk(session_id: str, history: StreamlitChatMessageHistory) -> None:
    """If Streamlit session is fresh but disk has messages, load them into StreamlitChatMessageHistory."""
    try:
        if history.messages:
            return
        disk_msgs = _load_messages_from_disk(session_id)
        if not disk_msgs:
            return
        for m in disk_msgs:
            if m.get("type") in ("human", "user"):
                history.add_user_message(m.get("content", ""))
            else:
                history.add_ai_message(m.get("content", ""))
        logger.info(f"[CHAT_STORE] hydrated {len(disk_msgs)} messages into session {session_id}")
    except Exception as e:
        logger.info(f"[CHAT_STORE] hydrate failed for {session_id}: {e}")

os.makedirs(str(CFG["LOG_DIR"]), exist_ok=True)
LOG_PATH = os.path.join(str(CFG["LOG_DIR"]), "app.log")

logger = logging.getLogger("trag")
logger.setLevel(logging.INFO)
if not logger.handlers:
    fh = logging.FileHandler(LOG_PATH, encoding="utf-8")
    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    fh.setFormatter(fmt)
    logger.addHandler(fh)


def _log_vectordb_status(vs: Chroma, prefix: str, added_chunks: int, reason: str) -> None:
    """(5)(14) ë²¡í„°DB ìƒíƒœ ë¡œê·¸"""
    try:
        total = vs._collection.count()  # type: ignore[attr-defined]
    except Exception:
        total = -1

    msg = f"[{prefix}] VectorDB updated: added_chunks={added_chunks}, total_vectors={total}, reason={reason}"
    logger.info(msg)
    # sidebarì—ë„ ë³´ì—¬ì£¼ë©´ ë””ë²„ê¹… í¸í•¨
    try:
        st.sidebar.info(msg)
    except Exception:
        pass


# =========================
# 2) Vector DB path (21) + fingerprints
# =========================


def _sanitize(s: str) -> str:
    s = (s or "").strip()
    s = s.replace("/", "_").replace(":", "_").replace(" ", "_")
    return s


def vectordb_dir(embed_model: str) -> str:
    # (21) ./vector_db/chroma_db_ollama_{ì„ë² ë”©ëª¨ë¸}
    name = f"chroma_db_ollama_{_sanitize(embed_model)}"
    p = os.path.join(CFG["VECTOR_DB_ROOT"], name)
    os.makedirs(p, exist_ok=True)
    return p


def fingerprints_path(embed_model: str) -> str:
    return os.path.join(vectordb_dir(embed_model), "fingerprints.json")


def load_fingerprints(embed_model: str) -> Dict[str, Dict[str, str]]:
    fp = fingerprints_path(embed_model)
    if not os.path.exists(fp):
        return {"pdf": {}, "news": {}}
    try:
        with open(fp, "r", encoding="utf-8") as f:
            data = json.load(f)
        data.setdefault("pdf", {})
        data.setdefault("news", {})
        return data
    except Exception:
        return {"pdf": {}, "news": {}}


def save_fingerprints(embed_model: str, data: Dict[str, Dict[str, str]]) -> None:
    fp = fingerprints_path(embed_model)
    os.makedirs(os.path.dirname(fp), exist_ok=True)
    with open(fp, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def file_fingerprint(path: str) -> str:
    st_ = os.stat(path)
    return f"{st_.st_size}:{int(st_.st_mtime)}"


# =========================
# 3) Vectorstore + embeddings + chunking
# =========================

@st.cache_resource
def get_embeddings(embed_model: str):
    return OllamaEmbeddings(model=embed_model)


@st.cache_resource
def get_vectorstore(embed_model: str) -> Chroma:
    # (6) vector db type configurable, but this implementation is chroma only
    if str(CFG["VECTOR_DB"]).lower() != "chroma":
        logger.info(f"VECTOR_DB='{CFG['VECTOR_DB']}' not supported yet. Fallback to 'chroma'.")
    return Chroma(
        persist_directory=vectordb_dir(embed_model),
        embedding_function=get_embeddings(embed_model),
    )


def split_docs(docs):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=int(CFG["CHUNK_SIZE"]),
        chunk_overlap=int(CFG["CHUNK_OVERLAP"]),
    )
    return splitter.split_documents(docs)


def delete_by_source(vs: Chroma, source_path: str) -> None:
    """ê°™ì€ sourceë¥¼ ê°€ì§„ ê¸°ì¡´ ë¬¸ì„œë¥¼ ì‚­ì œí•˜ê³  ì¬ì„ë² ë”© (ë³€ê²½ ë°˜ì˜)"""
    try:
        vs._collection.delete(where={"source": source_path})  # type: ignore[attr-defined]
    except Exception as e:
        logger.info(f"[VectorDB] delete(where=source) skipped/failed: {e}")


def add_documents(vs: Chroma, docs, prefix: str, reason: str) -> int:
    if not docs:
        return 0
    chunks = split_docs(docs)
    vs.add_documents(chunks)
    _log_vectordb_status(vs, prefix=prefix, added_chunks=len(chunks), reason=reason)
    return len(chunks)


# =========================
# 4) PDF upload + sync
# =========================


def list_pdfs() -> List[str]:
    return sorted(
        os.path.join(CFG["DATA_DIR"], n)
        for n in os.listdir(CFG["DATA_DIR"])
        if n.lower().endswith(".pdf")
    )


def save_uploaded_pdf(uploaded) -> Optional[str]:
    """(1) ì—…ë¡œë“œëœ PDFë¥¼ ./dataì— ì €ì¥"""
    if uploaded is None:
        return None
    name = uploaded.name
    if not name.lower().endswith(".pdf"):
        st.sidebar.error("PDF íŒŒì¼ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return None

    os.makedirs(CFG["DATA_DIR"], exist_ok=True)
    dst = os.path.join(CFG["DATA_DIR"], name)

    data = uploaded.getvalue()
    with open(dst, "wb") as f:
        f.write(data)

    st.sidebar.success(f"ì—…ë¡œë“œ ì™„ë£Œ: {name} â†’ {CFG['DATA_DIR']}")
    logger.info(f"[UPLOAD] pdf saved: {dst} ({len(data)} bytes)")
    return dst


def sync_pdf_dir(vs: Chroma, embed_model: str) -> int:
    """(3) ë³€ê²½ëœ PDFë§Œ ì„ë² ë”©"""
    fps = load_fingerprints(embed_model)
    before = dict(fps.get("pdf", {}))

    total_added = 0
    for p in list_pdfs():
        fp = file_fingerprint(p)
        if before.get(p) == fp:
            continue

        # ë³€ê²½/ì‹ ê·œì´ë©´ ê¸°ì¡´ source ì‚­ì œ í›„ ì¬ì¶”ê°€
        delete_by_source(vs, p)

        loader = PyPDFLoader(p)
        docs = loader.load()
        for d in docs:
            d.metadata = d.metadata or {}
            d.metadata["source"] = p
            d.metadata["source_type"] = "pdf"

        total_added += add_documents(vs, docs, prefix="PDF", reason=f"changed_or_new: {os.path.basename(p)}")
        fps["pdf"][p] = fp

    save_fingerprints(embed_model, fps)
    return total_added


# =========================
# 5) News crawl (Google News RSS) + save (dedup) + sync
# =========================

DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}


def google_news_rss_url(query: str) -> str:
    from urllib.parse import quote_plus

    q = quote_plus(query)
    return f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"


def strip_html(text: str) -> str:
    """HTML ì œê±°(ê°„ë‹¨)"""
    s = text or ""
    if BeautifulSoup is not None:
        try:
            s = BeautifulSoup(s, "html.parser").get_text(" ", strip=True)
        except Exception:
            s = re.sub(r"<[^>]+>", " ", s)
    else:
        s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def normalize_url(u: str) -> str:
    return (u or "").strip()


def representative_sentence(title: str, summary_html: str) -> str:
    # ê°„ë‹¨/ê²¬ê³ í•œ 1ë¬¸ì¥ ì¶”ì¶œ
    summary = strip_html(summary_html)
    if not summary:
        return (title or "").strip()

    m = re.search(r"[.!?](?:\s+|$)|ë‹¤\.(?:\s+|$)", summary)
    first = summary[: m.end()].strip() if m else summary.strip()

    if len(first) < 25:
        return f"{(title or '').strip()} - {first}".strip(" -")
    return first


def stable_news_id(title: str, link: str) -> str:
    # (13) ì¤‘ë³µ ë°©ì§€ key
    base = ((title or "").strip().lower() + "|" + (link or "").strip())
    return hashlib.sha256(base.encode("utf-8")).hexdigest()


def fetch_article_fulltext(url: str, timeout_sec: int) -> str:
    """ë‰´ìŠ¤ ë§í¬ì—ì„œ ê°€ëŠ¥í•œ í•œ 'ì „ì²´ ë³¸ë¬¸'ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ìš°ì„ ìˆœìœ„:
    1) trafilatura (ê°€ëŠ¥í•˜ë©´)ë¡œ ë³¸ë¬¸ ì¶”ì¶œ
    2) BeautifulSoupë¡œ <article> ë˜ëŠ” ë³¸ë¬¸ í›„ë³´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    3) ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

    ì£¼ì˜: ì‚¬ì´íŠ¸ ì •ì±…/êµ¬ì¡°/ì°¨ë‹¨ì— ë”°ë¼ ì „ì²´ ë³¸ë¬¸ì´ ì œí•œë  ìˆ˜ ìˆì–´, ì‹¤íŒ¨í•˜ë©´ RSS summaryë¡œ í´ë°±í•©ë‹ˆë‹¤.
    """
    url = normalize_url(url)
    if not url:
        return ""

    try:
        r = requests.get(url, headers=DEFAULT_HEADERS, timeout=timeout_sec, allow_redirects=True)
        r.raise_for_status()
        html = r.text
    except Exception as e:
        logger.info(f"[NEWS] article fetch failed: {url} ({e})")
        return ""

    # 1) trafilatura
    if trafilatura is not None:
        try:
            downloaded = trafilatura.extract(html, url=url, include_comments=False, include_tables=False)
            if downloaded:
                text = re.sub(r"\s+", " ", downloaded).strip()
                # ë„ˆë¬´ ì§§ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
                if len(text) >= 400:
                    return text
        except Exception as e:
            logger.info(f"[NEWS] trafilatura extract failed: {url} ({e})")

    # 2) BeautifulSoup heuristic
    if BeautifulSoup is not None:
        try:
            soup = BeautifulSoup(html, "html.parser")
            # remove scripts/styles
            for t in soup(["script", "style", "noscript"]):
                try:
                    t.decompose()
                except Exception:
                    pass

            # Prefer <article>
            article = soup.find("article")
            if article is not None:
                text = article.get_text(" ", strip=True)
            else:
                # Heuristic: collect paragraphs from common containers
                candidates = []
                for sel in [
                    "main",
                    "#content",
                    ".content",
                    ".article",
                    ".news",
                    ".post",
                    "body",
                ]:
                    node = soup.select_one(sel)
                    if node is None:
                        continue
                    ps = [p.get_text(" ", strip=True) for p in node.find_all("p")]
                    joined = " ".join([p for p in ps if p])
                    if len(joined) > len(" ".join(candidates)):
                        candidates = ps
                text = " ".join([p for p in candidates if p])

            text = re.sub(r"\s+", " ", text).strip()
            if len(text) >= 400:
                return text
        except Exception as e:
            logger.info(f"[NEWS] soup extract failed: {url} ({e})")

    return ""


def fetch_google_news(keyword: str) -> List[Dict[str, Any]]:
    url = google_news_rss_url(keyword)
    r = requests.get(url, headers=DEFAULT_HEADERS, timeout=int(CFG["NEWS_TIMEOUT_SEC"]))
    r.raise_for_status()
    feed = feedparser.parse(r.text)

    out: List[Dict[str, Any]] = []
    for e in feed.entries[: int(CFG["NEWS_MAX_ITEMS_PER_KEYWORD"])]:
        title = getattr(e, "title", "").strip()
        link = normalize_url(getattr(e, "link", "").strip())
        published = getattr(e, "published", "") or getattr(e, "updated", "")
        summary = getattr(e, "summary", "") or getattr(e, "description", "")

        fulltext = ""
        # ê°€ëŠ¥í•œ ê²½ìš° ë§í¬ ë³¸ë¬¸ì„ ê°€ì ¸ì™€ full text ì¶”ì¶œ
        if link:
            fulltext = fetch_article_fulltext(link, timeout_sec=int(CFG["NEWS_TIMEOUT_SEC"]))

        out.append(
            {
                "keyword": keyword,
                "title": title,
                "link": link,
                "published": published,
                "summary": summary,
                "fulltext": fulltext,
            }
        )
    return out


def ensure_news_dir():
    os.makedirs(CFG["NEWS_DIR"], exist_ok=True)


def news_index_path() -> str:
    return os.path.join(CFG["NEWS_DIR"], "news_index.json")


def load_news_index() -> Dict[str, str]:
    # news_id -> filename
    p = news_index_path()
    if not os.path.exists(p):
        return {}
    try:
        with open(p, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}


def save_news_index(idx: Dict[str, str]) -> None:
    with open(news_index_path(), "w", encoding="utf-8") as f:
        json.dump(idx, f, ensure_ascii=False, indent=2)


def save_news_items(items: List[Dict[str, Any]]) -> List[str]:
    """(10)(13) ë‰´ìŠ¤ txt ìƒì„± ë° ì¤‘ë³µ ë°©ì§€"""
    ensure_news_dir()
    idx = load_news_index()

    created: List[str] = []
    for e in items:
        title = (e.get("title") or "").strip()
        link = (e.get("link") or "").strip()
        published = (e.get("published") or "").strip()
        keyword = (e.get("keyword") or "").strip()
        summary = (e.get("summary") or "").strip()
        fulltext = (e.get("fulltext") or "").strip()

        nid = stable_news_id(title, link)
        if nid in idx:
            # ì´ë¯¸ ì €ì¥ëœ ë‰´ìŠ¤
            continue

        # íŒŒì¼ëª…ì€ ì„ì˜ë¡œ ìƒì„±(ìš”êµ¬ì‚¬í•­ 10)í•˜ì§€ë§Œ, indexë¡œ ì¤‘ë³µì„ ê´€ë¦¬
        fname = f"news_{uuid.uuid4().hex}.txt"
        path = os.path.join(CFG["NEWS_DIR"], fname)

        rep = representative_sentence(title, fulltext if fulltext else summary)

        body = strip_html(fulltext) if fulltext else strip_html(summary)
        with open(path, "w", encoding="utf-8") as f:
            f.write(f"# title: {title}\n")
            f.write(f"# url: {link}\n")
            f.write(f"# published: {published}\n")
            f.write(f"# keyword: {keyword}\n")
            f.write(f"# representative: {rep}\n")
            f.write(f"# has_fulltext: {bool(fulltext)}\n\n")
            f.write(rep + "\n\n")
            f.write("===== FULL CONTENT =====\n")
            f.write(body + "\n")

        logger.info(f"[NEWS] saved: {fname} fulltext={bool(fulltext)} len={len(body)}")

        idx[nid] = fname
        created.append(path)

    if created:
        save_news_index(idx)
        logger.info(f"[NEWS] created {len(created)} new files")

    return created


def crawl_news_once() -> List[str]:
    """(11) ì£¼ê¸° í¬ë¡¤ë§ ë‹¨ë°œ"""
    items: List[Dict[str, Any]] = []
    for kw in CFG["NEWS_KEYWORDS"]:
        try:
            items.extend(fetch_google_news(kw))
        except Exception as e:
            logger.info(f"[NEWS] fetch failed for '{kw}': {e}")
    return save_news_items(items)


def list_news_txts() -> List[str]:
    ensure_news_dir()
    return sorted(
        os.path.join(CFG["NEWS_DIR"], n)
        for n in os.listdir(CFG["NEWS_DIR"])
        if n.lower().endswith(".txt")
    )


def sync_news_dir(vs: Chroma, embed_model: str) -> int:
    """(12) ë³€ê²½ëœ/ì‹ ê·œ ë‰´ìŠ¤ txtë§Œ ì„ë² ë”©"""
    fps = load_fingerprints(embed_model)
    before = dict(fps.get("news", {}))

    total_added = 0
    for p in list_news_txts():
        fp = file_fingerprint(p)
        if before.get(p) == fp:
            continue

        delete_by_source(vs, p)

        loader = TextLoader(p, encoding="utf-8")
        docs = loader.load()
        for d in docs:
            d.metadata = d.metadata or {}
            d.metadata["source"] = p
            d.metadata["source_type"] = "news_text"

        total_added += add_documents(vs, docs, prefix="NEWS", reason=f"changed_or_new: {os.path.basename(p)}")
        fps["news"][p] = fp

    save_fingerprints(embed_model, fps)
    return total_added


# =========================
# 6) Periodic tasks in Streamlit
# =========================


def init_periodic_state():
    now = time.time()
    # ì²« ì‹¤í–‰(ì„¸ì…˜ ìµœì´ˆ ë¡œë“œ)ì—ì„œëŠ” ì£¼ê¸° ì‘ì—…ì„ ì¦‰ì‹œ ëŒë¦¬ì§€ ì•Šë„ë¡ 'í˜„ì¬ ì‹œê°'ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•˜ë©´ ì•±ì´ ëœ¨ëŠ” ì†ë„ê°€ ë¹¨ë¼ì§€ê³ , ì„¤ì •í•œ ê°„ê²©ì´ ì§€ë‚œ ë’¤ì—ë§Œ í¬ë¡¤ë§/ì„ë² ë”©ì´ ìˆ˜í–‰ë©ë‹ˆë‹¤.
    st.session_state.setdefault("_last_pdf_sync", now)
    st.session_state.setdefault("_last_news_crawl", now)
    st.session_state.setdefault("_last_news_sync", now)


def periodic_tasks(vs: Chroma, embed_model: str) -> None:
    init_periodic_state()
    now = time.time()

    # (4) PDF ì£¼ê¸° ì²´í¬
    if now - st.session_state["_last_pdf_sync"] >= int(CFG["PDF_SYNC_INTERVAL_SEC"]):
        try:
            added = sync_pdf_dir(vs, embed_model)
            logger.info(f"[PDF] periodic sync done. added_chunks={added}")
        except Exception as e:
            logger.info(f"[PDF] periodic sync failed: {e}")
        st.session_state["_last_pdf_sync"] = now

    # (11) ë‰´ìŠ¤ ì£¼ê¸° í¬ë¡¤ë§
    if now - st.session_state["_last_news_crawl"] >= int(CFG["NEWS_CRAWL_INTERVAL_SEC"]):
        try:
            created = crawl_news_once()
            logger.info(f"[NEWS] periodic crawl done. created_files={len(created)}")
        except Exception as e:
            logger.info(f"[NEWS] periodic crawl failed: {e}")
        st.session_state["_last_news_crawl"] = now

    # (12) ë‰´ìŠ¤ ì„ë² ë”© ë™ê¸°í™”
    if now - st.session_state["_last_news_sync"] >= int(CFG["NEWS_CRAWL_INTERVAL_SEC"]):
        try:
            added = sync_news_dir(vs, embed_model)
            logger.info(f"[NEWS] periodic sync done. added_chunks={added}")
        except Exception as e:
            logger.info(f"[NEWS] periodic sync failed: {e}")
        st.session_state["_last_news_sync"] = now


# --- LLM caching + Korean-only post-processing helpers ---
@st.cache_resource
def get_llm(model_name: str):
    # Deterministic output helps reduce language-mixing
    # Also bias toward Korean via system prompt and sampling controls.
    try:
        return ChatOllama(
            model=model_name,
            temperature=0.0,
            top_p=0.9,
            num_predict=512,
            repeat_penalty=1.1,
        )
    except TypeError:
        # Fallback for older langchain_ollama versions
        return ChatOllama(model=model_name, temperature=0.0, top_p=0.9)


_ALLOWED_ASCII_ACRONYMS = {
    "SDV", "LLM", "AI", "ISO", "IEC", "ASIL", "HARA", "FMEA", "FMEDA", "FTA", "GSN",
}


def _needs_korean_rewrite(text: str) -> bool:
    """Detect if the answer contains too much non-Korean text.

    - Allow short ASCII acronyms (SDV, ISO 26262, AI, LLM, etc.)
    - If there are many Latin words or CJK (non-Hangul) characters, request rewrite.
    - Be aggressive for any non-Hangul CJK, Japanese kana, Cyrillic, Turkish, or Latin words (>=3 letters, not in allowlist).
    """
    if not text:
        return False

    s = text.strip()
    hangul = len(re.findall(r"[ê°€-í£]", s))

    # Latin words (length >= 3) excluding allowed acronyms
    latin_words = re.findall(r"[A-Za-z]{3,}", s)
    latin_words_filtered = [w for w in latin_words if w.upper() not in _ALLOWED_ASCII_ACRONYMS]

    # Japanese Kana + CJK ideographs + Cyrillic etc. (anything we don't want mixed in)
    jp_kana = re.findall(r"[\u3040-\u30ff]", s)
    cjk_ideographs = re.findall(r"[\u4e00-\u9fff]", s)
    cyrillic = re.findall(r"[\u0400-\u04FF]", s)
    turkish = re.findall(r"[ÄŸÄÅŸÅÄ±Ä°Ã¶Ã–Ã¼Ãœ]", s)

    # If ANY Japanese kana appear, rewrite.
    if jp_kana:
        return True

    # If CJK ideographs appear together with Hangul, it's likely mixed-language (e.g., è‡ªå‹•é‹è»¢).
    if cjk_ideographs and hangul > 0:
        return True

    # If Cyrillic/Turkish letters appear, rewrite.
    if cyrillic or turkish:
        return True

    # If there are any non-allowed Latin words and some Hangul exists, rewrite.
    if latin_words_filtered and hangul > 0:
        return True

    # If Hangul is very scarce but foreign tokens exist, rewrite.
    if hangul < 10 and (latin_words_filtered or cjk_ideographs or cyrillic or turkish):
        return True

    return False


def _rewrite_to_korean_only(llm_model: str, answer: str) -> str:
    """Rewrite answer into natural Korean. Keep technical acronyms as-is."""
    llm = get_llm(llm_model)

    sys = (
        "ë‹¹ì‹ ì€ í•œêµ­ì–´ í¸ì§‘ìì´ì ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ 'ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´'ë¡œë§Œ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”. "
        "ì˜ì–´/ì¼ë³¸ì–´/ì¤‘êµ­ì–´/ë² íŠ¸ë‚¨ì–´/í„°í‚¤ì–´/ëŸ¬ì‹œì•„ì–´ ë“± ì–´ë–¤ ì™¸êµ­ì–´ ë¬¸ì¥ë„ ì„ì§€ ë§ˆì„¸ìš”. "
        "ì™¸êµ­ì–´ ë‹¨ì–´/ë¬¸ì¥/í•œì í‘œê¸°(ì˜ˆ: - é€²ã‚ã‚‹, è‡ªå‹•é‹è»¢, ç§ã®, expertise, Ã¶zellikle ë“±)ê°€ ìˆìœ¼ë©´ ì˜ë¯¸ë¥¼ ìœ ì§€í•œ ì±„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ ë°”ê¿” ì“°ì„¸ìš”. "
        "ë‹¤ë§Œ ê¸°ìˆ  ì•½ì–´/í‘œì¤€ëª…/ì œí’ˆëª…(ì˜ˆ: SDV, LLM, AI, ISO 26262, UNECE R155)ì€ í•„ìš”í•  ë•Œë§Œ ê·¸ëŒ€ë¡œ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
        "ë¬¸ì²´ëŠ” ì¡´ëŒ“ë§ë¡œ ê³µì†í•˜ê²Œ ìœ ì§€í•˜ê³ , ë¬¸ì¥ì€ ë§¤ë„ëŸ½ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì„¸ìš”."
    )

    prompt = ChatPromptTemplate.from_messages(
        [("system", sys), ("human", "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ì‹œ ì¨ ì£¼ì„¸ìš”:\n\n{answer}")]
    )

    try:
        msg = prompt.invoke({"answer": answer})
        out = llm.invoke(msg)
        return getattr(out, "content", str(out)).strip()
    except Exception:
        return answer


def ensure_korean_output(llm_model: str, answer: str) -> str:
    if _needs_korean_rewrite(answer):
        out = _rewrite_to_korean_only(llm_model, answer)
        # If still mixed, do one more pass (prevents stubborn mixed-script outputs)
        if out and _needs_korean_rewrite(out):
            out = _rewrite_to_korean_only(llm_model, out)
        return out
    return answer



# =========================
# 7) RAG chain
# =========================


def build_rag_chain(vs: Chroma, llm_model: str):
    retriever = vs.as_retriever(search_kwargs={"k": 5})

    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question which might reference context in the chat history, "
        "formulate a standalone question which can be understood without the chat history. "
        "Do NOT answer the question, just reformulate it if needed and otherwise return it as is."
    )

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    qa_system_prompt = (
        "ë‹¹ì‹ ì€ í•œêµ­ì–´ ì§ˆë¬¸-ë‹µë³€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ê²€ìƒ‰ëœ ë¬¸ì„œ ì¡°ê°(Context)ì„ ì°¸ê³ í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "- ë‹µë³€ì€ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•©ë‹ˆë‹¤.\n"
        "- ì˜ì–´/ì¼ë³¸ì–´/ì¤‘êµ­ì–´ ë“± ì™¸êµ­ì–´ ë¬¸ì¥ì€ ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤. ì™¸êµ­ì–´ê°€ í•„ìš”í•´ ë³´ì´ë©´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ ì„¤ëª…í•©ë‹ˆë‹¤.\n"
        "- ê¸°ìˆ  ì•½ì–´/í‘œì¤€ëª…/ì œí’ˆëª…(ì˜ˆ: SDV, ISO 26262, LLM, AI)ì€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ê·¸ëŒ€ë¡œ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        "- ê·¼ê±°ê°€ ë¶€ì¡±í•˜ë©´ ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  ë§í•©ë‹ˆë‹¤.\n"
        "- ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•´ ê³µì†í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.\n\n"
        "[Context]\n{context}"
    )

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    llm = get_llm(llm_model)

    # (7) ê²€ìƒ‰ ê¸°ë°˜ ì§ˆì˜ ì—…ë°ì´íŠ¸(íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì¬êµ¬ì„± ê°€ëŠ¥)
    if create_history_aware_retriever is not None:
        history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    else:
        # Fallback: retrieverëŠ” ë¬¸ìì—´ queryë¥¼ ë°›ìœ¼ë¯€ë¡œ inputë§Œ ì „ë‹¬
        history_aware_retriever = RunnableLambda(lambda x: x["input"]) | retriever

    def _format_docs(docs):
        return "\n\n".join(getattr(d, "page_content", str(d)) for d in (docs or []))

    def _build_inputs(x: Dict[str, Any]) -> Dict[str, Any]:
        # RunnableWithMessageHistoryê°€ ì£¼ì…í•˜ëŠ” historyëŠ” ê·¸ëŒ€ë¡œ ë³´ì¡´
        return {
            "input": x.get("input", ""),
            "history": x.get("history", []),
        }

    def _retrieve(x: Dict[str, Any]):
        # history-aware retrieverëŠ” dictë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
        return history_aware_retriever.invoke(x)

    def _to_qa_vars(x: Dict[str, Any]) -> Dict[str, Any]:
        docs = x.get("context_docs", []) or []
        return {
            "input": x.get("input", ""),
            "history": x.get("history", []),
            "context_docs": docs,
            "context": _format_docs(docs),
        }

    def _final(x: Dict[str, Any]) -> Dict[str, Any]:
        return {"answer": x.get("answer", ""), "context": x.get("context_docs", [])}

    rag_chain = (
        RunnableLambda(_build_inputs)
        .assign(context_docs=RunnableLambda(_retrieve))
        .assign(**{
            "context": RunnableLambda(lambda x: _format_docs(x.get("context_docs", []))),
        })
        .assign(
            answer=(
                qa_prompt
                | llm
                | RunnableLambda(lambda m: getattr(m, "content", str(m)))
            )
        )
        | RunnableLambda(_final)
    )

    return rag_chain


def build_pure_llm_chain(llm_model: str):
    #llm = ChatOllama(
    #    model=llm_model,
    #    temperature=0.0,
    #    top_p=0.9,
    #)
    llm = get_llm(llm_model)

    sys = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ í•œêµ­ì–´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë‹µë³€ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ê³ (ì™¸êµ­ì–´ ë¬¸ì¥ ê¸ˆì§€), ì¡´ëŒ“ë§ë¡œ ê³µì†í•˜ê²Œ ë‹µí•˜ì„¸ìš”."
    prompt_tpl = ChatPromptTemplate.from_messages(
        [("system", sys), MessagesPlaceholder("history"), ("human", "{input}")]
    )

    chain = (
        RunnablePassthrough
        .assign(
            prompt=RunnableLambda(
                lambda x: prompt_tpl.invoke({
                    "history": x.get("history", []),
                    "input": x.get("input", ""),
                })
            )
        )
        .assign(answer=RunnableLambda(lambda x: llm.invoke(x["prompt"])))
        | RunnableLambda(lambda x: {"answer": getattr(x["answer"], "content", str(x["answer"]))})
    )
    return chain


# =========================
# 8) Multi chat sessions UI
# =========================

# --- Chat title keyword extraction (titles derived from first message keywords) ---
import unicodedata

# Small Korean/English stopword set
_CHAT_TITLE_STOPWORDS = set([
    "the", "and", "or", "of", "in", "on", "at", "to", "for", "with", "a", "an", "is", "are", "was", "were", "be",
    "by", "as", "from", "that", "this", "it", "but", "if", "then", "so", "not", "do", "does", "did",
    "i", "you", "he", "she", "we", "they", "my", "your", "our", "their", "me", "him", "her", "them", "us",
    "can", "will", "would", "should", "could", "may", "might", "must",
    "how", "what", "when", "where", "who", "which", "why", "about",
    # Korean stopwords (common particles, pronouns, etc.)
    "ì˜", "ì´", "ê°€", "ì€", "ëŠ”", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ì—ê²Œ", "ê»˜", "ë¡œ", "ìœ¼ë¡œ", "ì™€", "ê³¼", "ë„", "ë§Œ",
    "ë³´ë‹¤", "ì²˜ëŸ¼", "ê¹Œì§€", "ë¶€í„°", "í•˜ê³ ", "ë³´ë‹¤", "ë§ˆë‹¤", "ë¼ë„", "ì´ë‚˜", "ë‚˜", "ë“ ì§€", "ì¡°ì°¨", "ë§ˆì €", "ë°–ì—",
    "ë°", "ë“±", "ë˜ëŠ”", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ê·¸ë˜ì„œ", "ì¦‰", "í˜¹ì€", "ë•Œë¬¸ì—", "ê·¸ëŸ¬ë¯€ë¡œ", "ë”°ë¼ì„œ",
    "ì €", "ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ì €í¬", "ë„ˆí¬", "ê·¸", "ê·¸ë…€", "ì´ê²ƒ", "ì €ê²ƒ", "ê·¸ê²ƒ", "ëˆ„êµ¬", "ë¬´ì—‡", "ì–´ë””", "ì–¸ì œ", "ì–´ë–»ê²Œ", "ì™œ",
])

def _extract_title_keywords(text: str, max_terms: int = 3) -> List[str]:
    """
    Extract up to max_terms representative keywords from the text for chat title.
    - Normalizes whitespace.
    - Extracts tokens using regex (Korean, alphanum, technical tokens).
    - Filters stopwords and very short tokens (len < 2 unless contains digit).
    - De-duplicates while preserving order.
    """
    if not text:
        return []
    text = re.sub(r"\s+", " ", text).strip()
    # Regex: Korean, alphanum, technical (e.g., ISO26262, SDV, R155)
    # Allow: í•œê¸€, ì˜ì–´, ìˆ«ì, ISO/SDV/R155 ë“±
    tokens = re.findall(r"[ê°€-í£]{2,}|[A-Za-z0-9]{2,}(?:[.-][A-Za-z0-9]+)*", text)
    # Lowercase for stopword filtering, but preserve original for output
    seen = set()
    result = []
    for tok in tokens:
        tok_norm = tok.lower()
        # Remove stopwords
        if tok_norm in _CHAT_TITLE_STOPWORDS:
            continue
        # Remove very short tokens unless contains digit (e.g. "R155")
        if len(tok) < 2 and not any(c.isdigit() for c in tok):
            continue
        # De-duplicate
        if tok_norm in seen:
            continue
        seen.add(tok_norm)
        result.append(tok)
        if len(result) >= max_terms:
            break
    return result

def _make_chat_title(user_text: str, ai_text: str = "") -> str:
    """
    Make a chat title using representative keywords from user_text (and ai_text if needed).
    Joins keywords with " Â· ". Fallback: "ìƒˆ ì±„íŒ…".
    """
    keywords = _extract_title_keywords(user_text, max_terms=3)
    if len(keywords) < 2 and ai_text:
        # Supplement from answer
        ai_kw = _extract_title_keywords(ai_text, max_terms=3)
        # Add only new keywords
        for k in ai_kw:
            if k not in keywords:
                keywords.append(k)
            if len(keywords) >= 3:
                break
    if keywords:
        return " Â· ".join(keywords)
    return "ìƒˆ ì±„íŒ…"


def init_chat_registry():
    # Load from disk first (survive browser refresh)
    if "chat_registry" not in st.session_state:
        disk_reg = _load_chat_registry_from_disk()
        st.session_state["chat_registry"] = disk_reg

    st.session_state.setdefault("chat_registry", {})  # session_id -> title

    # Ensure there is at least one chat
    if not st.session_state.get("chat_registry"):
        sid = str(uuid.uuid4())
        st.session_state["chat_registry"][sid] = "ìƒˆ ì±„íŒ…"
        st.session_state["active_chat_id"] = sid
        _save_chat_registry_to_disk(st.session_state["chat_registry"])
        return

    # Restore active chat id (prefer existing)
    if "active_chat_id" not in st.session_state:
        # Pick first chat deterministically
        first_sid = next(iter(st.session_state["chat_registry"].keys()))
        st.session_state["active_chat_id"] = first_sid


def new_chat():
    reg: Dict[str, str] = st.session_state["chat_registry"]
    sid = str(uuid.uuid4())
    # New chats are initially titled "ìƒˆ ì±„íŒ…"
    reg[sid] = "ìƒˆ ì±„íŒ…"
    st.session_state["active_chat_id"] = sid
    _save_chat_registry_to_disk(reg)


def sidebar_chat_list():
    st.sidebar.header("ì±„íŒ… ëª©ë¡")

    busy = bool(st.session_state.get("_is_generating", False))
    active = st.session_state["active_chat_id"]

    # Busy notice (do NOT disable buttons; allow switch requests)
    if busy:
        st.sidebar.info("ë‹µë³€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì§€ê¸ˆ ì „í™˜/ìƒˆ ì±„íŒ…ì„ ëˆ„ë¥´ë©´ 'ì „í™˜ ì˜ˆì•½'ìœ¼ë¡œ ì²˜ë¦¬ë˜ê³ , ë‹µë³€ì´ ëë‚˜ë©´ ìë™ ì „í™˜ë©ë‹ˆë‹¤.")

    # New chat button: allowed even when busy (will be scheduled)
    if st.sidebar.button("+ ìƒˆ ì±„íŒ…", key="btn_new_chat"):
        if busy:
            sid = str(uuid.uuid4())
            st.session_state["chat_registry"][sid] = "ìƒˆ ì±„íŒ…"
            _save_chat_registry_to_disk(st.session_state["chat_registry"])
            # schedule switching after generation
            st.session_state["_switch_to_chat_id"] = sid
            st.session_state["_switch_requested_at"] = time.time()
            st.sidebar.success("ìƒˆ ì±„íŒ…ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ë‹µë³€ ìƒì„±ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ ì „í™˜ë©ë‹ˆë‹¤.")
        else:
            new_chat()
            st.rerun()

    reg: Dict[str, str] = st.session_state["chat_registry"]

    for sid, title in reg.items():
        label = ("âœ… " if sid == active else "") + title
        if st.sidebar.button(label, key=f"chat_{sid}"):
            if busy:
                st.session_state["_switch_to_chat_id"] = sid
                st.session_state["_switch_requested_at"] = time.time()
                st.sidebar.warning("ì „í™˜ì„ ì˜ˆì•½í–ˆìŠµë‹ˆë‹¤. ë‹µë³€ ìƒì„±ì´ ì™„ë£Œë˜ë©´ ìë™ ì „í™˜ë©ë‹ˆë‹¤.")
                # rerun to reflect selection intent in UI immediately
                st.rerun()
            else:
                # save current chat messages before switching
                try:
                    cur = st.session_state.get("active_chat_id")
                    if cur:
                        _save_messages_to_disk(cur, get_chat_history(cur).messages)
                except Exception:
                    pass
                st.session_state["active_chat_id"] = sid
                st.rerun()

    st.sidebar.divider()


# =========================
# 9) Streamlit App
# =========================

st.set_page_config(page_title="TRAG BaseRAG_v3", layout="wide")

# Runtime flags: prevent rerun while generating
st.session_state.setdefault("_is_generating", False)
st.session_state.setdefault("_gen_started_at", None)
# Busy/switch control
st.session_state.setdefault("_busy_chat_id", None)
st.session_state.setdefault("_switch_to_chat_id", None)
st.session_state.setdefault("_switch_requested_at", None)

# If a previous run was interrupted (e.g., user switched chat / stopped the app while generating),
# Streamlit might not execute `finally:` blocks and `_is_generating` can remain True forever.
# This guard resets stale "generating" state so the UI won't be stuck.
GEN_STALE_SEC = int(CFG.get("GENERATION_STALE_SEC", 180))
now_ts = time.time()
if st.session_state.get("_is_generating", False):
    started = st.session_state.get("_gen_started_at")
    busy_chat = st.session_state.get("_busy_chat_id")
    if (started is None) or (busy_chat is None) or (now_ts - float(started) > GEN_STALE_SEC):
        logger.info(
            f"[GUARD] Reset stale generating state. started_at={started} now={now_ts} stale_sec={GEN_STALE_SEC}"
        )
        st.session_state["_is_generating"] = False
        st.session_state["_gen_started_at"] = None
        st.session_state["_busy_chat_id"] = None
        st.session_state["_switch_to_chat_id"] = None
        st.session_state["_switch_requested_at"] = None

# If config file missing, show guidance
if not os.path.exists(CONFIG_FILE):
    st.sidebar.warning("./config/config.pyê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤.")

# Sidebar chat list
init_chat_registry()
sidebar_chat_list()


# Apply scheduled switch only when not generating
if not st.session_state.get("_is_generating", False):
    sid = st.session_state.get("_switch_to_chat_id")
    if sid and sid in st.session_state.get("chat_registry", {}):
        st.session_state["active_chat_id"] = sid
        st.session_state["_switch_to_chat_id"] = None
        st.session_state["_switch_requested_at"] = None
        st.rerun()

# If a switch request is very old, clear it (prevents permanent "reserved" state)
req_at = st.session_state.get("_switch_requested_at")
if req_at and (time.time() - float(req_at) > int(CFG.get("GENERATION_STALE_SEC", 180))):
    st.session_state["_switch_to_chat_id"] = None
    st.session_state["_switch_requested_at"] = None


# Sidebar: upload + manual sync
st.sidebar.header("ë°ì´í„°")
up = st.sidebar.file_uploader("PDF ì—…ë¡œë“œ", type=["pdf"], accept_multiple_files=False)
if up is not None:
    save_uploaded_pdf(up)

if st.sidebar.button("ì§€ê¸ˆ PDF/NEWS ë™ê¸°í™”"):
    st.session_state["_force_sync"] = True
    st.rerun()

# Sidebar: show current config
st.sidebar.header("ì„¤ì •")
st.sidebar.caption(f"LLM={CFG['LLM_MODEL']} / EMB={CFG['EMBED_MODEL']}")
st.sidebar.caption(f"chunk={CFG['CHUNK_SIZE']} overlap={CFG['CHUNK_OVERLAP']}")
st.sidebar.caption(f"pdf_interval={CFG['PDF_SYNC_INTERVAL_SEC']}s news_interval={CFG['NEWS_CRAWL_INTERVAL_SEC']}s")
st.sidebar.caption("ë‰´ìŠ¤ í‚¤ì›Œë“œ:")
for k in CFG["NEWS_KEYWORDS"]:
    st.sidebar.caption(f"- {k}")
st.sidebar.caption(f"config: {CONFIG_FILE}")

# Optional autorefresh so periodic tasks can run without user input
if bool(CFG["AUTO_REFRESH_ENABLED"]) and (not st.session_state.get("_is_generating", False)):
    try:
        from streamlit_autorefresh import st_autorefresh
        st_autorefresh(interval=int(CFG["AUTO_REFRESH_TICK_SEC"]) * 1000, key="auto_refresh")
    except Exception:
        pass

# Vectorstore
vs = get_vectorstore(CFG["EMBED_MODEL"])

# Sync is intentionally NOT executed on first load.
# Only run sync when the user explicitly requests it (button), or via periodic tasks.
if st.session_state.pop("_force_sync", False):
    try:
        ap = sync_pdf_dir(vs, CFG["EMBED_MODEL"])
        created = crawl_news_once()
        an = sync_news_dir(vs, CFG["EMBED_MODEL"])
        logger.info(
            f"[FORCE] sync done. pdf_added_chunks={ap}, news_created_files={len(created)}, news_added_chunks={an}"
        )
        # Give immediate UI feedback
        st.sidebar.success(
            f"ë™ê¸°í™” ì™„ë£Œ: PDF chunks +{ap}, News files +{len(created)}, News chunks +{an}"
        )
    except Exception as e:
        logger.info(f"[FORCE] sync failed: {e}")
        st.sidebar.error(f"ë™ê¸°í™” ì‹¤íŒ¨: {e}")

# Periodic tasks (4)(11) - will run based on configured intervals
periodic_tasks(vs, CFG["EMBED_MODEL"])

# Status
pdf_count = len(list_pdfs())
news_count = len(list_news_txts())

st.header("TRAG Chatbot v01 ğŸ’¬ğŸ“š")
st.caption(
    f"PDF: {pdf_count}ê°œ / News txt: {news_count}ê°œ / VectorDB: {CFG['VECTOR_DB']} ({vectordb_dir(CFG['EMBED_MODEL'])})"
)
st.caption(f"LLM: {CFG['LLM_MODEL']} / Embedding: {CFG['EMBED_MODEL']} / Log: {LOG_PATH}")

# Choose chain: (9) no docs -> pure LLM
no_docs = (pdf_count == 0 and news_count == 0)

if no_docs:
    chain = build_pure_llm_chain(CFG["LLM_MODEL"])
else:
    chain = build_rag_chain(vs, CFG["LLM_MODEL"])

# Per-session history (16)(17)(18)(19)
session_id = st.session_state["active_chat_id"]
logger.info(f"[SESSION] active_chat_id={session_id}")

def get_chat_history(sid: str) -> StreamlitChatMessageHistory:
    # One independent StreamlitChatMessageHistory per chat session
    return StreamlitChatMessageHistory(key=f"chat_messages_{sid}")

chat_history = get_chat_history(session_id)
_hydrate_history_from_disk(session_id, chat_history)

# Render history
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

# Run chat
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.chat_message("human").write(prompt)
    # ë‹µë³€ ìƒì„± ì‹œì‘
    st.session_state["_is_generating"] = True
    st.session_state["_gen_started_at"] = time.time()
    st.session_state["_busy_chat_id"] = st.session_state.get("active_chat_id")
    answer = ""

    try:
        with st.chat_message("ai"):
            with st.spinner("Thinking..."):
                if no_docs:
                    resp = chain.invoke({"input": prompt, "history": chat_history.messages})
                    answer = resp.get("answer", "")
                    answer = ensure_korean_output(CFG["LLM_MODEL"], answer)
                    st.write(answer)
                else:
                    conversational = RunnableWithMessageHistory(
                        chain,
                        get_chat_history,
                        input_messages_key="input",
                        history_messages_key="history",
                        output_messages_key="answer",
                    )
                    try:
                        resp = conversational.invoke(
                            {"input": prompt},
                            config={"configurable": {"session_id": session_id}},
                        )
                    except Exception as e:
                        logger.info(f"[CHAT] RAG invoke failed: {e}")
                        resp = {"answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", "context": []}

                    answer = resp.get("answer", "")
                    answer = ensure_korean_output(CFG["LLM_MODEL"], answer)
                    st.write(answer)

                    with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                        for doc in resp.get("context", []) or []:
                            src = doc.metadata.get("source", "unknown source")
                            st.markdown(src, help=doc.page_content)

        # (16) í˜„ì¬ ì±„íŒ…ì°½ ì¢…ë£Œ ì „ê¹Œì§€ ë§¥ë½ ìœ ì§€ (LLM ë‹µë³€ì€ ë²¡í„° DBì— ì €ì¥í•˜ì§€ ì•ŠìŒ)
        #chat_history.add_user_message(prompt)
        #chat_history.add_ai_message(answer)

        # --- Auto-rename chat after first exchange using keywords ---
        reg: Dict[str, str] = st.session_state.get("chat_registry", {})
        current_title = reg.get(session_id, "")
        # Only update if current title is "ìƒˆ ì±„íŒ…" or starts with "Chat" (backward compatibility)
        if current_title == "ìƒˆ ì±„íŒ…" or (current_title or "").startswith("Chat"):
            new_title = _make_chat_title(prompt, answer)
            st.session_state["chat_registry"][session_id] = new_title
            _save_chat_registry_to_disk(st.session_state["chat_registry"])
            # Do NOT st.rerun() here; sidebar will reflect on next rerun

        # âœ… Persist messages to disk so browser refresh won't lose them
        try:
            _save_messages_to_disk(session_id, chat_history.messages)
        except Exception:
            pass

    finally:
        st.session_state["_is_generating"] = False
        st.session_state["_gen_started_at"] = None
        st.session_state["_busy_chat_id"] = None

        # Apply scheduled switch if any
    sid = st.session_state.get("_switch_to_chat_id")
    if sid and sid in st.session_state.get("chat_registry", {}):
        st.session_state["active_chat_id"] = sid
        _save_chat_registry_to_disk(st.session_state["chat_registry"])
        st.session_state["_switch_to_chat_id"] = None
        st.session_state["_switch_requested_at"] = None
        st.rerun()

# Footer
st.sidebar.divider()
st.sidebar.caption(f"ë¡œê·¸ íŒŒì¼: {LOG_PATH}")
